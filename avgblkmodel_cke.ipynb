{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Motivation:\n",
    "    \n",
    "Given the information on gpu and kernel, model the concurrent kernel runtime.\n",
    "\n",
    "Shortcomings:\n",
    "* assume the kernels are in 1D grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import *\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeviceInfo():\n",
    "    def __init__(self, sm_num=0, sharedmem_per_sm=0, reg_per_sm=0, maxthreads_per_sm=0):\n",
    "        self.sm_num = sm_num\n",
    "        self.sharedmem_per_sm = sharedmem_per_sm # bytes\n",
    "        self.reg_per_sm = reg_per_sm\n",
    "        self.maxthreads_per_sm = maxthreads_per_sm\n",
    "        \n",
    "class KernelInfo():\n",
    "    def __init__(self, blockDim=0, gridDim=0, reg_per_thread=0, sharedmem_per_blk=0, runtime_ms = 0):\n",
    "        self.blockDim = blockDim\n",
    "        self.gridDim = gridDim\n",
    "        self.reg_per_thread = reg_per_thread\n",
    "        self.sharedmem_per_blk =  sharedmem_per_blk\n",
    "        self.runtime_ms = runtime_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MaxBLK_Per_SM(Gpu, Kern):\n",
    "    \"\"\"\n",
    "    Compute the max blocks on one SM\n",
    "    \"\"\"\n",
    "    warp_size = 32\n",
    "    DeviceLimit = Gpu.maxthreads_per_sm / 32\n",
    "    \n",
    "    blocks_by_sm = DeviceLimit\n",
    "    \n",
    "    if Kern.sharedmem_per_blk > 0:\n",
    "        blocks_by_sm = floor(Gpu.sharedmem_per_sm / float(Kern.sharedmem_per_blk)) # int operation\n",
    "        \n",
    "    blocks_by_reg = floor(Gpu.reg_per_sm / float(Kern.reg_per_thread * Kern.blockDim))\n",
    "    \n",
    "    blocks_by_threads = floor(Gpu.maxthreads_per_sm / float(Kern.blockDim))\n",
    "    \n",
    "    # maxblks_per_sm\n",
    "    return min([blocks_by_sm, blocks_by_reg, blocks_by_threads])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_avgblktime(Gpu, kernel):\n",
    "    max_blk_per_sm = MaxBLK_Per_SM(Gpu, kernel)\n",
    "    print('max blk per sm = {}'.format(max_blk_per_sm))\n",
    "    \n",
    "    # max blocks that can be launhed on gpu at once time\n",
    "    # if there are more blocks, they will wait for the next iteration\n",
    "    # each SM starts and finishes at the same time\n",
    "    # all the blocks on that SM starts and ends at the same time\n",
    "    block_per_iteration = Gpu.sm_num * max_blk_per_sm\n",
    "\n",
    "    iterations = ceil(kernel.gridDim / block_per_iteration) # total iterations\n",
    "    #print 'iterations ' + str(iterations)\n",
    "\n",
    "    # divide the kernel runtime by the number of iterations will be the avg block exeuction time for our model\n",
    "    avg_blk_time = kernel.runtime_ms / float(iterations)\n",
    "    #print('avg block execution time (ms) : {}'.format(avg_blk_time))\n",
    "    \n",
    "    return avg_blk_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup GPU info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gtx950 = DeviceInfo()\n",
    "gtx950.sm_num = 2\n",
    "gtx950.sharedmem_per_sm = 49152\n",
    "gtx950.reg_per_sm = 65536\n",
    "gtx950.maxthreads_per_sm = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup kernel info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simulate kernel number\n",
    "kernel_num = 3\n",
    "\n",
    "kernels = [KernelInfo() for i in range(kernel_num)]\n",
    "\n",
    "kernels[0].blockDim = 512\n",
    "kernels[0].gridDim = 10\n",
    "kernels[0].reg_per_thread = 28\n",
    "kernels[0].sharedmem_per_blk= 0\n",
    "kernels[0].runtime_ms = 0.057249\n",
    "\n",
    "\n",
    "kernels[1].blockDim = 512\n",
    "kernels[1].gridDim = 10\n",
    "kernels[1].reg_per_thread = 28\n",
    "kernels[1].sharedmem_per_blk= 0\n",
    "kernels[1].runtime_ms = 0.057249 * 0.5   # assume half runtime\n",
    "\n",
    "kernels[2].blockDim = 512\n",
    "kernels[2].gridDim = 10\n",
    "kernels[2].reg_per_thread = 28\n",
    "kernels[2].sharedmem_per_blk= 0\n",
    "kernels[2].runtime_ms = 0.057249 * 2  # assume x2 runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute average block execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max blk per sm = 4.0\n",
      "max blk per sm = 4.0\n",
      "max blk per sm = 4.0\n"
     ]
    }
   ],
   "source": [
    "avg_blk_time_list = []\n",
    "\n",
    "for kid in range(kernel_num):\n",
    "    avg_blk_time_list.append(compute_avgblktime(gtx950, kernels[kid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0286245, 0.01431225, 0.057249]"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_blk_time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we model the multiple kernel concurrent execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class sm_stat:\n",
    "    def __init__(self, thread=0, reg=0, sharedmem = 0, full=0, batch = 1):\n",
    "        self.thread = thread\n",
    "        self.reg= reg\n",
    "        self.sharedmem = sharedmem\n",
    "        self.full = full\n",
    "        self.batch = batch\n",
    "\n",
    "    def init(self, Gpu):\n",
    "        self.thread = Gpu.maxthreads_per_sm\n",
    "        self.reg = Gpu.reg_per_sm\n",
    "        self.sharedmem = Gpu.sharedmem_per_sm\n",
    "        self.full = 0 \n",
    "        self.batch = 1\n",
    "    \n",
    "    def replenish(self, Gpu):\n",
    "        self.thread = Gpu.maxthreads_per_sm\n",
    "        self.reg = Gpu.reg_per_sm\n",
    "        self.sharedmem = Gpu.sharedmem_per_sm\n",
    "        self.full = 0 \n",
    "        self.batch += 1 # add\n",
    "        \n",
    "    def Rm(self, Kern):\n",
    "        \"\"\"\n",
    "        Remove the kernel block occupied resource by adding back them.\n",
    "        \"\"\"\n",
    "        self.thread += Kern.blockDim\n",
    "        self.reg += Kern.reg_per_thread * Kern.blockDim\n",
    "        self.sharedmem += Kern.sharedmem_per_blk\n",
    "\n",
    "    def Allocate_block(self, Kern):\n",
    "        self.thread -= Kern.blockDim\n",
    "        self.reg -= Kern.reg_per_thread * Kern.blockDim\n",
    "        self.sharedmem -= Kern.sharedmem_per_blk\n",
    "\n",
    "        \n",
    "def check_sm_resource(current_sm, block_info):\n",
    "    enough_thread = current_sm.thread >= block_info.blockDim\n",
    "    enough_reg = current_sm.reg >= (block_info.reg_per_thread * block_info.blockDim)\n",
    "    enough_sm = current_sm.sharedmem >= block_info.sharedmem_per_blk\n",
    "    \n",
    "    allocate = 0\n",
    "    if enough_thread and enough_reg and enough_sm:\n",
    "        allocate = 1\n",
    "    \n",
    "    return allocate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Search_block_start(df_sm_trace, current_kernel_id):\n",
    "    \"\"\"\n",
    "    Read the sm_trace table, find out all the active blocks on current sm, look for the earliest start\n",
    "    \"\"\"\n",
    "    \n",
    "    #df_active = df_sm_trace.loc[(df_sm_trace['active'] == 1) & (df_sm_trace['kernel_id'] == current_kernel_id)]\n",
    "    df_active = df_sm_trace.loc[df_sm_trace['active'] == 1]\n",
    "    blk2start = df_active['block_start'].max() # find the closest block\n",
    "    \n",
    "    df_active_current_kernel = df_active.loc[df_active['kernel_id'] == current_kernel_id]\n",
    "    if not df_active_current_kernel.empty:\n",
    "        blk2start = df_active_current_kernel['block_start'].max()  # find the closest blk for current kernel\n",
    "    \n",
    "    return blk2start "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling the execution and record the trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init SM status\n",
    "sm_num = gtx950.sm_num\n",
    "sms = [sm_stat() for i in range(sm_num)]\n",
    "\n",
    "for i in range(sm_num):\n",
    "    sms[i].init(gtx950)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a trace table to record all the block trace: using pd dataframe\n",
    "trace_table = pd.DataFrame(columns=['sm_id', 'block_id', 'block_start', 'block_end', 'batch_id', 'kernel_id', 'active'])\n",
    "\n",
    "# have a trace table for each sm\n",
    "sm_trace = [trace_table for x in range(gtx950.sm_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print len(sm_trace)\n",
    "# print sm_trace[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sm2start = 0\n",
    "\n",
    "for i in range(kernel_num):\n",
    "    kern = kernels[i] # schedule current kernel on the device\n",
    "    kernel_blocks = int(kern.gridDim) # total block for current kern\n",
    "    \n",
    "    last_block_on_sm = 0\n",
    "    \n",
    "    #print sm2start\n",
    "    \n",
    "    for bid in range(kernel_blocks):\n",
    "        # find out which sm to allocate\n",
    "        sm_id = (bid + sm2start) % sm_num\n",
    "        \n",
    "        # check whether current sm has enough resources to host the block\n",
    "        to_allocate_another_block = check_sm_resource(sms[sm_id], kern)\n",
    "        \n",
    "        #print('kernel {} , bik {}, smid {}, to_allocate_another_block = {}'.format(i, bid, sm_id, to_allocate_another_block))\n",
    "\n",
    "        #-------------------------------------------\n",
    "        # There is no more resources to host the blk, consider SM is full now\n",
    "        # we need to (1) decide how many blks to retire (2) when to start current blk\n",
    "        #-------------------------------------------\n",
    "        if to_allocate_another_block == 0:\n",
    "            # find the list blocks to retire\n",
    "            df_sm = sm_trace[sm_id]\n",
    "            df_activeblk = df_sm.loc[df_sm['active'] == 1]\n",
    "            #print df_activeblk\n",
    "            \n",
    "            # print sm_trace[sm_id]\n",
    "            \n",
    "            blkend_min = df_activeblk['block_end'].min()\n",
    "            # print('blkend_min = {}'.format(blkend_min))\n",
    "            \n",
    "            df_blk2end = df_activeblk.loc[df_activeblk['block_end'] == blkend_min]\n",
    "            for index, row in df_blk2end.iterrows():\n",
    "                sm_trace[sm_id].loc[index]['active'] = 0 # retire the block\n",
    "                sms[sm_id].Rm(kern) # free the block resource\n",
    "            \n",
    "            #print('blkid {}'.format(bid))\n",
    "            \n",
    "            # print sm_trace[sm_id]\n",
    "            \n",
    "            # after retiring some blocks, we have resources to allocate current block\n",
    "            sms[sm_id].Allocate_block(kern)\n",
    "            \n",
    "            \n",
    "            block_start = blkend_min # when prev blks end, current block starts\n",
    "            block_end = block_start + avg_blk_time_list[i] # add avgblktime for currrent kernel\n",
    "            \n",
    "            # update the trace table\n",
    "            sm_trace[sm_id] = sm_trace[sm_id].append({'sm_id': sm_id, \n",
    "                                                      'block_id': bid, \n",
    "                                                      'block_start': block_start,\n",
    "                                                      'block_end' : block_end,\n",
    "                                                      'batch_id': sms[sm_id].batch,\n",
    "                                                      'kernel_id': i,\n",
    "                                                      'active': 1}, ignore_index=True)\n",
    "        \n",
    "        #----------------------------------\n",
    "        # there is enough resource to host the current block\n",
    "        #----------------------------------\n",
    "        if to_allocate_another_block == 1:\n",
    "            # allocate the block on current sm\n",
    "            sms[sm_id].Allocate_block(kern)\n",
    "\n",
    "            # register the block in the trace table\n",
    "            block_start = None\n",
    "\n",
    "            # if current sm trace table is empty, start from 0\n",
    "            # else find the blocks that will end soon, and retire them\n",
    "            if sm_trace[sm_id].empty:\n",
    "                block_start = 0\n",
    "            else:\n",
    "                # read the sm_trace table, find out all the active blocks on current sm, look for the earliest start\n",
    "                block_start = Search_block_start(sm_trace[sm_id], i)\n",
    "\n",
    "            block_end = block_start + avg_blk_time_list[i]\n",
    "\n",
    "            # add the current block info to the current sm\n",
    "            sm_trace[sm_id] = sm_trace[sm_id].append({'sm_id': sm_id, \n",
    "                                                      'block_id': bid, \n",
    "                                                      'block_start': block_start,\n",
    "                                                      'block_end' : block_end,\n",
    "                                                      'batch_id': sms[sm_id].batch,\n",
    "                                                      'kernel_id': i,\n",
    "                                                      'active': 1}, ignore_index=True)\n",
    "        last_block_on_sm = sm_id\n",
    "        \n",
    "    # end of running previous kernel blocks\n",
    "    sm2start = (last_block_on_sm + 1) % sm_num # start from next smd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sm_trace[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sm_trace[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leiming/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0db6ac9210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def Plot_sm_trace(df_sm):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # color to pick\n",
    "    color_list = ['r', 'k', 'b', 'g', 'c', 'm', 'y', '#cc0000', '#cc6600', 'cc9900',\n",
    "                  '#cc3300', '#cccc00', '#99cc00', '#66cc00', '#00cccc', '#0033cc', \n",
    "                  '#6600cc', '#cc00cc', '#ff99cc', 'ffcc00']\n",
    "    \n",
    "    total_color = len(color_list) \n",
    "    \n",
    "    kern_ids = df_sm['kernel_id'].unique()\n",
    "    \n",
    "    x0_dd = {}\n",
    "    x1_dd = {}\n",
    "    y0_dd = {}\n",
    "    \n",
    "    ylim_max = 0\n",
    "    \n",
    "    for kid in kern_ids:\n",
    "        offset = 0.1 * kid\n",
    "        df_kern = df_sm.loc[df_sm['kernel_id'] == kid] # get the data for current kernel on\n",
    "        df_kern['y_axis'] = pd.Series(np.arange(1,len(df_kern.index)+1) + offset, \n",
    "                                      index=df_kern.index) # adding y_axis label\n",
    "    \n",
    "        x0_dd[kid] = df_kern['block_start']\n",
    "        x1_dd[kid] = df_kern['block_end']\n",
    "        y0_dd[kid] = df_kern['y_axis']\n",
    "\n",
    "        current_ymax = max(df_kern['y_axis']) + 1\n",
    "    \n",
    "        if ylim_max < current_ymax:\n",
    "            ylim_max = current_ymax\n",
    "    \n",
    "\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0, ylim_max])\n",
    "\n",
    "    for kid in kern_ids:\n",
    "        cid = int(kid) % total_color\n",
    "        plt.hlines(y0_dd[kid], x0_dd[kid], x1_dd[kid], lw=2, color=color_list[cid])\n",
    "\n",
    "    \n",
    "                                \n",
    "# #     plt.title('Memory Bound')\n",
    "    plt.xlabel('Time(clocks)')\n",
    "    plt.ylabel('Blocks')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "#---------------------\n",
    "# plot\n",
    "#---------------------\n",
    "df_sm = sm_trace[0]\n",
    "\n",
    "Plot_sm_trace(df_sm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
